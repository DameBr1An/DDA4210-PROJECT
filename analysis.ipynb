{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: phil jackson's reputed, patented triangle offense. what are the patterns and reads to the offense and what are the benefits from the perspective of a coach and/or player? what type of players do you need for this offense to be successful?\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace()\n",
    "args.generate_model=\"D:\\\\DDA4210\\\\facebookopt-1.3b\"\n",
    "args.util_model=\"D:\\\\DDA4210\\\\gpt\"\n",
    "args.prompts_name=\"lfqa.json\"\n",
    "args.prompt_index = 10\n",
    "args.use_gpu=True\n",
    "args.prompt_max_length = None\n",
    "args.max_new_tokens=200\n",
    "args.gamma=0.25\n",
    "args.delta=2.0\n",
    "args.detection_z_threshold=4.0\n",
    "args.generation_seed=42\n",
    "args.use_sampling=True\n",
    "args.sampling_temp=0.7\n",
    "args.n_beams=1\n",
    "args.normalizers=\"\"\n",
    "args.ignore_repeated_ngrams=False\n",
    "\n",
    "model, tokenizer, device, pplmodel, ppltokenizer = utils.load_model(args)\n",
    "\n",
    "original_answer, input_text = utils.load_prompts(args)\n",
    "print('prompt: ' + input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m without_wm, with_wm\u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#######################################\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated without watermark: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m without_wm)\n",
      "File \u001b[1;32md:\\DDA4210\\DDA4210-PROJECT\\utils.py:171\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(prompt, args, model, device, tokenizer)\u001b[0m\n\u001b[0;32m    167\u001b[0m     gen_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(num_beams\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mn_beams))\n\u001b[0;32m    169\u001b[0m output_without_watermark \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[1;32m--> 171\u001b[0m output_with_watermark \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs, logits_processor\u001b[38;5;241m=\u001b[39mLogitsProcessorList([watermark_processor]))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mis_decoder_only_model:\n\u001b[0;32m    173\u001b[0m     output_without_watermark \u001b[38;5;241m=\u001b[39m output_without_watermark[:,tokd_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\transformers\\generation\\utils.py:1575\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1567\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1568\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1569\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1570\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1572\u001b[0m     )\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1575\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   1576\u001b[0m         input_ids,\n\u001b[0;32m   1577\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1578\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1579\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1580\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1581\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1582\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1583\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[0;32m   1584\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1585\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1586\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1587\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1588\u001b[0m     )\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1593\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1594\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1600\u001b[0m     )\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\transformers\\generation\\utils.py:2710\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2707\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m   2709\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[1;32m-> 2710\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2711\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_warper(input_ids, next_token_scores)\n\u001b[0;32m   2713\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\transformers\\generation\\logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32md:\\DDA4210\\DDA4210-PROJECT\\watermarkbase.py:64\u001b[0m, in \u001b[0;36mWatermarkLogitsWarper.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rindex \u001b[38;5;129;01min\u001b[39;00m red_index:\n\u001b[0;32m     63\u001b[0m     new_logits[\u001b[38;5;241m0\u001b[39m,rindex] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(new_logits[\u001b[38;5;241m0\u001b[39m,rindex])\u001b[38;5;241m/\u001b[39m(green_sum\u001b[38;5;241m+\u001b[39mred_sum)\n\u001b[1;32m---> 64\u001b[0m new_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# new_logits = torch.exp(new_logits+self.strength)/(green_sum+red_sum)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# with open('greenlist.txt', 'a') as f:\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#     max_index = torch.argmax(new_logits, dim=1)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m#         f.write(str(-1) + ' ')\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_logits\n",
      "\u001b[1;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "without_wm, with_wm= utils.generate(input_text, \n",
    "                                        args, \n",
    "                                        model=model, \n",
    "                                        device=device, \n",
    "                                        tokenizer=tokenizer)\n",
    "print('#######################################')\n",
    "print('generated without watermark: ' + without_wm)\n",
    "print('#######################################')\n",
    "print('generated with watermark: ' + with_wm)\n",
    "rewritten_wm = utils.attack(with_wm)\n",
    "print('#######################################')\n",
    "print('generated with watermark: ' + rewritten_wm)\n",
    "refined_wm = utils.refine(with_wm)\n",
    "print('#######################################')\n",
    "print('refined with watermark: ' + refined_wm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_wm_detection = utils.detect(without_wm, \n",
    "                                    args, \n",
    "                                    device=device, \n",
    "                                    model = model,\n",
    "                                    tokenizer=tokenizer)\n",
    "with_wm_detection = utils.detect(with_wm, \n",
    "                                args, \n",
    "                                device=device, \n",
    "                                model = model,\n",
    "                                tokenizer=tokenizer)\n",
    "rewritten_with_wm_detection = utils.detect(rewritten_wm, \n",
    "                                        args, \n",
    "                                        device=device, \n",
    "                                        model = model,\n",
    "                                        tokenizer=tokenizer)\n",
    "print('detect finished')\n",
    "\n",
    "print('#######################################')\n",
    "print('watermark words:' , with_wm_detection[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_without_wm = utils.compute_ppl(without_wm, \n",
    "                                    args,\n",
    "                                    model=pplmodel,\n",
    "                                    device=device, \n",
    "                                    tokenizer=ppltokenizer)\n",
    "ppl_with_wm = utils.compute_ppl(with_wm,\n",
    "                                args,\n",
    "                                model=pplmodel,\n",
    "                                device=device, \n",
    "                                tokenizer=ppltokenizer)\n",
    "ppl_rewritten_with_wm = utils.compute_ppl(rewritten_wm,\n",
    "                                args,\n",
    "                                model=pplmodel,\n",
    "                                device=device, \n",
    "                                tokenizer=ppltokenizer)\n",
    "print('compute perplexity finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = {}\n",
    "analysis['gamma'] = args.gamma\n",
    "analysis['delta'] = args.delta\n",
    "analysis['z_threshold'] = args.detection_z_threshold\n",
    "\n",
    "analysis['T_with_watermark'] = with_wm_detection[0][1]\n",
    "analysis['z_with_watermark'] = with_wm_detection[2][1]\n",
    "analysis['p_with_watermark'] = with_wm_detection[3][1]\n",
    "analysis['prediction_with_watermark'] = with_wm_detection[6][1]\n",
    "# analysis['confidence_with_watermark'] = with_wm_detection[7][1]\n",
    "analysis['ppl_with_watermark'] = ppl_with_wm\n",
    "\n",
    "analysis['T_without_watermark'] = without_wm_detection[0][1]\n",
    "analysis['z_without_watermark'] = without_wm_detection[2][1]\n",
    "analysis['p_without_watermark'] = without_wm_detection[3][1]\n",
    "analysis['prediction_without_watermark'] = without_wm_detection[6][1]\n",
    "analysis['ppl_without_watermark'] = ppl_without_wm\n",
    "\n",
    "analysis['T_attack'] = rewritten_with_wm_detection[0][1]\n",
    "analysis['z_attack'] = rewritten_with_wm_detection[2][1]\n",
    "analysis['p_attack'] = rewritten_with_wm_detection[3][1]\n",
    "analysis['prediction_attack'] = rewritten_with_wm_detection[6][1]\n",
    "analysis['ppl_attack'] = ppl_rewritten_with_wm\n",
    "\n",
    "print(analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
