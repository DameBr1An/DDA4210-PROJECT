{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 加载预训练的 GPT-2 模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"D:\\\\DDA4210\\\\gpt\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"D:\\\\DDA4210\\\\gpt\")\n",
    "\n",
    "# 定义函数来计算困惑度并替换单词\n",
    "def calculate_perplexity_and_replace(text):\n",
    "    # 分词\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    # 在句子开头添加起始符号\n",
    "    tokenized_text = ['<|endoftext|>'] + tokenized_text\n",
    "    # 将单词转换为索引\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # 转换为张量\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "\n",
    "    # 使用模型预测下一个单词的概率分布\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # 计算困惑度\n",
    "    perplexities = []\n",
    "    for i, word in enumerate(tokenized_text):\n",
    "        word_logits = logits[0, i]  # 获取当前单词的概率分布\n",
    "        word_index = tokenizer.convert_tokens_to_ids(word)\n",
    "        word_probability = torch.softmax(word_logits, dim=-1)[word_index].item()  # 获取当前单词的概率\n",
    "        perplexity = 1 / word_probability  # 计算困惑度\n",
    "        perplexities.append(perplexity)\n",
    "\n",
    "    # 找出困惑度过高的单词\n",
    "    high_perplexity_words = [tokenized_text[i] for i, perplexity in enumerate(perplexities) if perplexity > 1]\n",
    "\n",
    "    # 对困惑度过高的单词进行替换\n",
    "    for word in high_perplexity_words:\n",
    "        # 替换成其他单词或者通过某种方法重新生成\n",
    "        pass\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "# 示例文本\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# 计算困惑度并替换单词\n",
    "processed_text = calculate_perplexity_and_replace(text)\n",
    "print(processed_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Charon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms: ['happy', 'felicitous', 'happy', 'glad', 'happy', 'happy', 'well-chosen']\n",
      "Antonyms: ['unhappy']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# 查找单词的同义词\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(\"Synonyms:\", synonyms)\n",
    "\n",
    "# 查找单词的反义词\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "print(\"Antonyms:\", antonyms)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
