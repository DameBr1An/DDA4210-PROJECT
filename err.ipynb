{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 加载预训练的 GPT-2 模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"D:\\\\DDA4210\\\\gpt\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"D:\\\\DDA4210\\\\gpt\")\n",
    "\n",
    "# 定义函数来计算困惑度并替换单词\n",
    "def calculate_perplexity_and_replace(text):\n",
    "    # 分词\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    # 在句子开头添加起始符号\n",
    "    tokenized_text = ['<|endoftext|>'] + tokenized_text\n",
    "    # 将单词转换为索引\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # 转换为张量\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "\n",
    "    # 使用模型预测下一个单词的概率分布\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # 计算困惑度\n",
    "    perplexities = []\n",
    "    for i, word in enumerate(tokenized_text):\n",
    "        word_logits = logits[0, i]  # 获取当前单词的概率分布\n",
    "        word_index = tokenizer.convert_tokens_to_ids(word)\n",
    "        word_probability = torch.softmax(word_logits, dim=-1)[word_index].item()  # 获取当前单词的概率\n",
    "        perplexity = 1 / word_probability  # 计算困惑度\n",
    "        perplexities.append(perplexity)\n",
    "\n",
    "    # 找出困惑度过高的单词\n",
    "    high_perplexity_words = [tokenized_text[i] for i, perplexity in enumerate(perplexities) if perplexity > 1]\n",
    "\n",
    "    # 对困惑度过高的单词进行替换\n",
    "    for word in high_perplexity_words:\n",
    "        # 替换成其他单词或者通过某种方法重新生成\n",
    "        pass\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "# 示例文本\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# 计算困惑度并替换单词\n",
    "processed_text = calculate_perplexity_and_replace(text)\n",
    "print(processed_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Charon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms: ['happy', 'felicitous', 'happy', 'glad', 'happy', 'happy', 'well-chosen']\n",
      "Antonyms: ['unhappy']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# 查找单词的同义词\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(\"Synonyms:\", synonyms)\n",
    "\n",
    "# 查找单词的反义词\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "print(\"Antonyms:\", antonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -35.8890,  -35.2049,  -39.1336,  ...,  -42.4869,  -41.8197,\n",
      "           -36.0383],\n",
      "         [-107.7291, -108.0175, -113.2968,  ..., -116.4646, -115.7443,\n",
      "          -110.8654],\n",
      "         [-100.5390,  -99.8514, -103.7539,  ..., -105.0177, -107.3317,\n",
      "          -102.0780],\n",
      "         [ -71.9370,  -72.7245,  -76.2084,  ...,  -82.9281,  -81.7860,\n",
      "           -73.6416],\n",
      "         [-104.6989, -105.5694, -111.0104,  ..., -116.2477, -115.0036,\n",
      "          -106.4377],\n",
      "         [-127.4615, -125.7557, -125.7914,  ..., -133.5118, -134.6318,\n",
      "          -119.2069]]])\n",
      "tensor(318)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 318 is out of bounds for dimension 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDDA4210\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDDA4210\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m word_perplexities \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_word_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Perplexities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, word_perplexities)\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36mcalculate_word_perplexity\u001b[1;34m(model, tokenizer, text)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(target_id)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 计算困惑度\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 将困惑度添加到列表中\u001b[39;00m\n\u001b[0;32m     40\u001b[0m word_perplexities\u001b[38;5;241m.\u001b[39mappend(perplexity\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[1;34m(logits, target_id)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m计算单个词的困惑度\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    perplexity: 单个词的困惑度，标量\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 选择实际下一个词的logits\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m target_logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 计算交叉熵损失\u001b[39;00m\n\u001b[0;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(target_logits\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor([target_id]))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 318 is out of bounds for dimension 1 with size 6"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_word_perplexity(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "    计算文本中每个单词的困惑度\n",
    "\n",
    "    Args:\n",
    "        model: 训练好的语言模型\n",
    "        tokenizer: 分词器\n",
    "        text: 输入的文本字符串\n",
    "\n",
    "    Returns:\n",
    "        word_perplexities: 包含每个单词困惑度的列表\n",
    "    \"\"\"\n",
    "    # 分词\n",
    "    tokens = tokenizer.encode(text)\n",
    "    # print(tokens)\n",
    "    # 添加特殊标记和转换为张量\n",
    "    # input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor(tokens).unsqueeze(0)  # 添加批次维度\n",
    "    # print(input_ids)\n",
    "    # 初始化空列表用于存储每个单词的困惑度\n",
    "    word_perplexities = []\n",
    "    \n",
    "    # 使用模型生成每个单词的概率分布并计算困惑度\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(tokens)):\n",
    "            # 获取当前位置之前的文本\n",
    "            context = input_ids[:i+1]\n",
    "            # 生成下一个词的概率分布\n",
    "            logits = model(context).logits\n",
    "            print(logits)\n",
    "            # 获取当前位置的实际下一个词的索引\n",
    "            target_id = input_ids[0, i+1]\n",
    "            print(target_id)\n",
    "            # 计算困惑度\n",
    "            perplexity = calculate_perplexity(logits, target_id)\n",
    "            # 将困惑度添加到列表中\n",
    "            word_perplexities.append(perplexity.item())\n",
    "    \n",
    "    return word_perplexities\n",
    "\n",
    "def calculate_perplexity(logits, target_id):\n",
    "    \"\"\"\n",
    "    计算单个词的困惑度\n",
    "\n",
    "    Args:\n",
    "        logits: 模型生成的logits张量，形状为(batch_size, vocab_size)\n",
    "        target_id: 实际的下一个词的索引\n",
    "\n",
    "    Returns:\n",
    "        perplexity: 单个词的困惑度，标量\n",
    "    \"\"\"\n",
    "    # 选择实际下一个词的logits\n",
    "    target_logits = logits[0, target_id]\n",
    "    # 计算交叉熵损失\n",
    "    loss = F.cross_entropy(target_logits.unsqueeze(0), torch.tensor([target_id]))\n",
    "    # 计算指数损失\n",
    "    perplexity = torch.exp(loss)\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "# 示例用法\n",
    "text = \"This is an example sentence.\"\n",
    "# 模型和分词器应根据实际情况替换为你的模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"D:\\\\DDA4210\\\\gpt\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"D:\\\\DDA4210\\\\gpt\")\n",
    "word_perplexities = calculate_word_perplexity(model, tokenizer, text)\n",
    "print(\"Word Perplexities:\", word_perplexities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.94070154811348"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "tokd_inputs = tokenizer.encode(\"This is an example sentence.\", return_tensors=\"pt\", add_special_tokens=True, truncation=True).to('cpu')\n",
    "# tokd_inputs = tokenizer.convert_tokens_to_ids(tokd_inputs)\n",
    "tokd_labels = tokd_inputs.clone().detach()\n",
    "outputs = model(input_ids=tokd_inputs, labels=tokd_labels)\n",
    "loss = outputs.loss\n",
    "perplexity = math.exp(loss)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 973.3511934110394\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# 定义文本\n",
    "text = \"This is an example sentence.\"\n",
    "\n",
    "# 使用分词器对文本进行分词，并添加特殊标记\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# 使用模型预测下一个词的概率分布\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# 计算困惑度\n",
    "loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "perplexity = math.exp(loss)/len(input_ids[0])\n",
    "\n",
    "print(\"Perplexity:\", perplexity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
