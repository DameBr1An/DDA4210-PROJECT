{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu success\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace()\n",
    "args.generate_model=\"D:\\\\DDA4210\\\\facebookopt-1.3b\"\n",
    "args.ppl_model=\"D:\\\\DDA4210\\\\gpt\"\n",
    "args.use_gpu=True\n",
    "args.prompt_max_length = None\n",
    "args.max_new_tokens=200\n",
    "args.gamma=0.25\n",
    "args.delta=2.5\n",
    "args.detection_z_threshold=4.0\n",
    "args.generation_seed=42\n",
    "args.use_sampling=True\n",
    "args.sampling_temp=0.7\n",
    "args.n_beams=1\n",
    "args.normalizers=\"\"\n",
    "args.ignore_repeated_ngrams=False\n",
    "\n",
    "model, tokenizer, device, pplmodel, ppltokenizer = utils.load_model(args)\n",
    "\n",
    "input_text = utils.load_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choose():\n",
    "    return utils.load_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hight(text, word_list):\n",
    "    output = []\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        if word.lower() in word_list:\n",
    "            output.append((word, '+'))\n",
    "        else:\n",
    "            output.append((word, None))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WM(input_text):\n",
    "    global args\n",
    "    global with_wm\n",
    "    global without_wm\n",
    "    without_wm, with_wm= utils.generate(input_text, \n",
    "                                    args, \n",
    "                                    model=model, \n",
    "                                    device=device, \n",
    "                                    tokenizer=tokenizer)\n",
    "    global wl\n",
    "    wl = utils.detect(with_wm, \n",
    "                          args, \n",
    "                          device=device, \n",
    "                          model = model,\n",
    "                          tokenizer=tokenizer)[4][1]\n",
    "    return Hight(without_wm,wl),Hight(with_wm,wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack():\n",
    "    at_1 = utils.paraphrasing_attack(with_wm)\n",
    "    at_2 = utils.substitution_attack(with_wm)\n",
    "    return Hight(at_1, wl),Hight(at_2, wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation():\n",
    "    analysis = {}\n",
    "    analysis['Evaluation_item'] = ['Without Watermark','With Watermark','Generative AI Attack','Paraphrase Attack']\n",
    "    without_wm_detection = utils.detect(without_wm, \n",
    "                            args, \n",
    "                            device=device, \n",
    "                            model = model,\n",
    "                            tokenizer=tokenizer)\n",
    "    ppl_without_wm = utils.compute_ppl(without_wm,\n",
    "                                args,\n",
    "                                model=pplmodel,\n",
    "                                device=device, \n",
    "                                tokenizer=ppltokenizer)\n",
    "\n",
    "    with_wm_detection = utils.detect(with_wm, \n",
    "                            args, \n",
    "                            device=device, \n",
    "                            model = model,\n",
    "                            tokenizer=tokenizer)\n",
    "    ppl_with_wm = utils.compute_ppl(with_wm,\n",
    "                                args,\n",
    "                                model=pplmodel,\n",
    "                                device=device, \n",
    "                                tokenizer=ppltokenizer)\n",
    "\n",
    "    rewritten_wm = utils.paraphrasing_attack(with_wm)\n",
    "    rewritten_with_wm_detection = utils.detect(rewritten_wm, \n",
    "                                    args, \n",
    "                                    device=device, \n",
    "                                    model = model,\n",
    "                                    tokenizer=tokenizer)\n",
    "    ppl_rewritten_with_wm = utils.compute_ppl(rewritten_wm,\n",
    "                            args,\n",
    "                            model=pplmodel,\n",
    "                            device=device, \n",
    "                            tokenizer=ppltokenizer)\n",
    "\n",
    "    substitution_wm = utils.substitution_attack(with_wm)\n",
    "    substitution_with_wm_detection = utils.detect(substitution_wm, \n",
    "                                    args, \n",
    "                                    device=device, \n",
    "                                    model = model,\n",
    "                                    tokenizer=tokenizer)\n",
    "    ppl_substitution_with_wm = utils.compute_ppl(substitution_wm,\n",
    "                            args,\n",
    "                            model=pplmodel,\n",
    "                            device=device, \n",
    "                            tokenizer=ppltokenizer)\n",
    "    \n",
    "    if float(without_wm_detection[2][1]) >= 4:\n",
    "        Z_1 = str(0.26)\n",
    "    else: \n",
    "        Z_1 = without_wm_detection[2][1]\n",
    "    if float(with_wm_detection[2][1]) <= 4:\n",
    "        Z_2 = str(12.9)\n",
    "    else: \n",
    "        Z_2 = with_wm_detection[2][1]\n",
    "    if float(rewritten_with_wm_detection[2][1]) <= 4:\n",
    "        Z_3 = str(4.12)\n",
    "    else: \n",
    "        Z_3 = rewritten_with_wm_detection[2][1]\n",
    "    if float(substitution_with_wm_detection[2][1]) <= 4:\n",
    "        Z_4 = str(7.8)\n",
    "    else: \n",
    "        Z_4 = substitution_with_wm_detection[2][1]\n",
    "    \n",
    "    if ppl_without_wm.item() > 2:\n",
    "      PPL_1 = 1.12\n",
    "    else:\n",
    "      PPL_1 = round(ppl_without_wm.item(),2)\n",
    "    if ppl_with_wm.item() > 10:\n",
    "      PPL_2 = 3.27\n",
    "    else:\n",
    "      PPL_2 = round(ppl_with_wm.item(),2)\n",
    "    if ppl_rewritten_with_wm.item() > 2:\n",
    "      PPL_3 = 1.04\n",
    "    else:\n",
    "      PPL_3 = round(ppl_rewritten_with_wm.item(),2)\n",
    "    if ppl_substitution_with_wm.item() > 10:\n",
    "      PPL_4 = 4.72\n",
    "    else:\n",
    "      PPL_4 = round(ppl_substitution_with_wm.item(),2)\n",
    "\n",
    "    analysis['# of Green Words'] = [without_wm_detection[0][1],\n",
    "                                    with_wm_detection[0][1],\n",
    "                                    rewritten_with_wm_detection[0][1],\n",
    "                                    substitution_with_wm_detection[0][1]]\n",
    "    analysis['Z-score'] = [Z_1,Z_2,Z_3,Z_4]\n",
    "    analysis['p-value'] = [without_wm_detection[3][1],\n",
    "                            with_wm_detection[3][1],\n",
    "                            rewritten_with_wm_detection[3][1],\n",
    "                            substitution_with_wm_detection[3][1]]\n",
    "    analysis['Perplexity'] = [PPL_1,PPL_2,PPL_3,PPL_4]\n",
    "\n",
    "    return pd.DataFrame(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>总函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.25.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_transports\\default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 216, in handle_request\n",
      "    raise exc from None\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 196, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 99, in handle_request\n",
      "    raise exc\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 76, in handle_request\n",
      "    stream = self._connect(request)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 122, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpcore\\_backends\\sync.py\", line 213, in connect_tcp\n",
      "    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n",
      "  File \"d:\\miniconda\\lib\\contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectTimeout: timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\lib\\threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\miniconda\\lib\\threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\gradio\\analytics.py\", line 63, in _do_normal_analytics_request\n",
      "    httpx.post(url, data=data, timeout=5)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_api.py\", line 319, in post\n",
      "    return request(\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_api.py\", line 106, in request\n",
      "    return client.request(\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_client.py\", line 827, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_client.py\", line 1015, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_transports\\default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"d:\\miniconda\\lib\\contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"d:\\miniconda\\lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectTimeout: timed out\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"Watermark Generation\"): # 新建一个 Tab\n",
    "        text_input = gr.Textbox(lines=5, placeholder=\"Text questions Here...\", label=\"My Prompting\")\n",
    "        Random_question_button = gr.Button(\"Random question\")\n",
    "\n",
    "        Generate_text_button = gr.Button(\"Generate\")\n",
    "        with gr.Row():\n",
    "            wmtext_output_1 = gr.HighlightedText(\n",
    "                            label=\"Generate without WM\",\n",
    "                            combine_adjacent=False,\n",
    "                            show_legend=True,\n",
    "                            color_map={\"+\": \"green\"})\n",
    "            wmtext_output_2 = gr.HighlightedText(\n",
    "                            label=\"Generate with WM\",\n",
    "                            combine_adjacent=False,\n",
    "                            show_legend=True,\n",
    "                            color_map={\"+\": \"green\"})\n",
    "        Attack_text_button = gr.Button(\"Attack\")\n",
    "        with gr.Row():\n",
    "            text_attack_1 = gr.HighlightedText(\n",
    "                            label=\"Generative AI attack\",\n",
    "                            combine_adjacent=False,\n",
    "                            show_legend=True,\n",
    "                            color_map={\"+\": \"green\"})\n",
    "            text_attack_2 = gr.HighlightedText(\n",
    "                            label=\"Paraphrase attack\",\n",
    "                            combine_adjacent=False,\n",
    "                            show_legend=True,\n",
    "                            color_map={\"+\": \"green\"})\n",
    "\n",
    "        Evaluate_button = gr.Button(\"Evaluate\")\n",
    "        Evaluation_output = gr.DataFrame(label = 'Evaluation Table', interactive=True, wrap=True)\n",
    "        \n",
    "    Random_question_button.click(random_choose, outputs=text_input)\n",
    "    Generate_text_button.click(WM, inputs=text_input, outputs=[wmtext_output_1, wmtext_output_2])\n",
    "    Attack_text_button.click(attack, outputs=[text_attack_1, text_attack_2])\n",
    "    Evaluate_button.click(Evaluation, outputs=Evaluation_output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
