{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace()\n",
    "args.generate_model=\"D:\\\\DDA4210\\\\facebookopt-1.3b\"\n",
    "args.ppl_model=\"D:\\\\DDA4210\\\\gpt\"\n",
    "args.prompts_name=\"lfqa.json\"\n",
    "args.use_gpu=True\n",
    "args.prompt_max_length = None\n",
    "args.max_new_tokens=200\n",
    "args.min_new_tokens=50\n",
    "args.gamma=0.25\n",
    "args.delta=2.0\n",
    "args.detection_z_threshold=4.0\n",
    "args.generation_seed=42\n",
    "args.normalizers=\"\"\n",
    "args.ignore_repeated_ngrams=False\n",
    "\n",
    "model, tokenizer, device, pplmodel, ppltokenizer = utils.load_model(args)\n",
    "\n",
    "input_text = utils.load_prompts()\n",
    "print('prompt: ' + input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "generated without watermark:   I'm not bashing the Romans; my point is that blaming them is a huge copout.\n",
      "Romans love white guilt. That's why we fight today. We took our white inheritance and appropriated it for our own uses. We stripped apart other people's stolen legacy and I suspect we're fucking corrupt now too. I'm not being racist, I'm just telling you what would happen if we were British not Romans.\n",
      "#######################################\n",
      "generated with watermark: \n",
      "\n",
      "You can chastise Latin culture for changing history to suit that man’s need to identify as an accuser – when Pheonilus and the telling by Caesar were forced upon Orosius and Cato by men who may have written Shakespeare’s laurel wreath; history is what makes, or holds back, culture. But perhaps history is who we identify as the victors?\n",
      "\n",
      "A quick rant here: if there is even a hint that Pheonilus is me, you have to treat me with kid gloves.\n",
      "\n",
      "I also think everyone who got hit by some terrible enough renaissance red beard to be imprisoned half a century ago is entitled to some semblance of humanity because of his 100% credibility despite some spelling flaws here and there in Pheonius accounts that at least have to be addressed lest we picture the guy as reversing history – or worse still, as some labyrinthine scheme to make Pope Pius appear more virtuous than that last night of gestation through rape that surely\n",
      "#######################################\n",
      "rewritten with watermark: Latin culture has been known to alter history to cater to the needs of those who identify as accusers. This is evident in the cases of Pheonilus and Caesar, whose narratives were imposed on Orosius and Cato by individuals who may have been responsible for Shakespeare's success. History plays a significant role in shaping culture, and it can either propel or hinder progress. However, it is worth noting that the victors are often the ones who dictate how history is recorded. On a side note, if Pheonilus is mistaken for me, I would appreciate being treated with caution. Additionally, individuals who were imprisoned during the renaissance era due to their beliefs should be accorded some level of respect and dignity, given their unwavering credibility. Any spelling errors in Pheonilus' accounts should be addressed to avoid any misinterpretation of historical events or any attempts to manipulate them for personal gain.\n",
      "#######################################\n",
      "refined with watermark: You behind chastise Latin culture for changing history to suit that man’s necessitate to identify type_A Associate_in_Nursing accuser – when Pheonilus and the secernate by Caesar were forced upon Orosius and Cato away serviceman World_Health_Organization may have written Shakespeare’s laurel wreath; story is what makes, or holds back, culture. But perhaps history is World_Health_Organization we identify As the victors? A quick rant here: if there comprise even amp speck that Pheonilus is me, you have to regale me with kid gloves. iodine too imagine everyone who got shoot by some terrible enough Renaissance reddened beard to be imprisoned half angstrom one_C agone is entitled to some semblance of humanity because of his 100% credibility despite some spelling flaws here and there in Pheonius business_relationship that at least consume to be addressed lest we picture the guy as reverse chronicle – or sorry still, angstrom_unit some labyrinthine scheme to make Pope Pius appear Sir_Thomas_More virtuous than that last Night of gestation through rape that surely\n"
     ]
    }
   ],
   "source": [
    "without_wm, with_wm= utils.generate(input_text, \n",
    "                                        args, \n",
    "                                        model=model, \n",
    "                                        device=device, \n",
    "                                        tokenizer=tokenizer)\n",
    "print('#######################################')\n",
    "print('generated without watermark: ' + without_wm)\n",
    "print('#######################################')\n",
    "print('generated with watermark: ' + with_wm)\n",
    "paraphrasing_wm = utils.paraphrasing_attack(with_wm)\n",
    "print('#######################################')\n",
    "print('rewritten with watermark: ' + paraphrasing_wm)\n",
    "substitution_wm = utils.substitution_attack(with_wm)\n",
    "print('#######################################')\n",
    "print('refined with watermark: ' + substitution_wm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_wm_detection = utils.detect(without_wm, \n",
    "                                    args, \n",
    "                                    device=device, \n",
    "                                    model = model,\n",
    "                                    tokenizer=tokenizer)\n",
    "with_wm_detection = utils.detect(with_wm, \n",
    "                                args, \n",
    "                                device=device, \n",
    "                                model = model,\n",
    "                                tokenizer=tokenizer)\n",
    "rewritten_with_wm_detection = utils.detect(paraphrasing_wm, \n",
    "                                        args, \n",
    "                                        device=device, \n",
    "                                        model = model,\n",
    "                                        tokenizer=tokenizer)\n",
    "print('detect finished')\n",
    "\n",
    "print('#######################################')\n",
    "print('watermark words:' , with_wm_detection[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_without_wm = utils.compute_ppl(without_wm, \n",
    "                                    args,\n",
    "                                    model=pplmodel,\n",
    "                                    device=device, \n",
    "                                    tokenizer=ppltokenizer)\n",
    "ppl_with_wm = utils.compute_ppl(with_wm,\n",
    "                                args,\n",
    "                                model=pplmodel,\n",
    "                                device=device, \n",
    "                                tokenizer=ppltokenizer)\n",
    "ppl_rewritten_with_wm = utils.compute_ppl(paraphrasing_wm,\n",
    "                                args,\n",
    "                                model=pplmodel,\n",
    "                                device=device, \n",
    "                                tokenizer=ppltokenizer)\n",
    "print('compute perplexity finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = {}\n",
    "analysis['gamma'] = args.gamma\n",
    "analysis['delta'] = args.delta\n",
    "analysis['z_threshold'] = args.detection_z_threshold\n",
    "\n",
    "analysis['T_with_watermark'] = with_wm_detection[0][1]\n",
    "analysis['z_with_watermark'] = with_wm_detection[2][1]\n",
    "analysis['p_with_watermark'] = with_wm_detection[3][1]\n",
    "analysis['prediction_with_watermark'] = with_wm_detection[6][1]\n",
    "# analysis['confidence_with_watermark'] = with_wm_detection[7][1]\n",
    "analysis['ppl_with_watermark'] = ppl_with_wm\n",
    "\n",
    "analysis['T_without_watermark'] = without_wm_detection[0][1]\n",
    "analysis['z_without_watermark'] = without_wm_detection[2][1]\n",
    "analysis['p_without_watermark'] = without_wm_detection[3][1]\n",
    "analysis['prediction_without_watermark'] = without_wm_detection[6][1]\n",
    "analysis['ppl_without_watermark'] = ppl_without_wm\n",
    "\n",
    "analysis['T_attack'] = rewritten_with_wm_detection[0][1]\n",
    "analysis['z_attack'] = rewritten_with_wm_detection[2][1]\n",
    "analysis['p_attack'] = rewritten_with_wm_detection[3][1]\n",
    "analysis['prediction_attack'] = rewritten_with_wm_detection[6][1]\n",
    "analysis['ppl_attack'] = ppl_rewritten_with_wm\n",
    "\n",
    "print(analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
